{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZMhNd1YtIDJ"
   },
   "source": [
    "# Overview\n",
    "\n",
    "## Maximum liklihood estimate\n",
    "Choosing greedily has tradeoffs\n",
    "\n",
    "## Explore vs. Exploit Dilemma\n",
    "\n",
    "Exploration - Collect data\n",
    "Exploitation - Select choice with the highest maximum liklihood estimate (i.e. highest win rate)\n",
    "\n",
    "## Algorithms\n",
    "- Greedy - Always pick the one with the highest maximum liklihood estimate\n",
    "- Epsilon Greedy - Small probability\n",
    "- Optimistic Initial Values - \n",
    "- UCB1 (Upper Confidence Bound) - \n",
    "- Thompson Sampling - Bayesian bandit method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avzFGTU44ho-"
   },
   "source": [
    "# Epsilon Greedy\n",
    "\n",
    "Pure greedy is a bad approach, because you could easily converge on a local maxima and never try other options again.\n",
    "\n",
    "The Epsilon Greedy algorithm will play the option with the highest liklihood, except for some fixed percentage of time (epsilon), where it will try another random option using a uniform distribution.\n",
    "\n",
    "## Decaying Epsilon\n",
    "Epsilon Greedy gives us an opportunity to consider other options early on in the testing lifecycle. But in the long run, if one variation is clearly better than all the others, but we're still randomly selecting from the others we will have a suboptimal outcome.\n",
    "\n",
    "We can use a decay function to make epsilon shrink over time. Here are some decay functions that can be used:\n",
    "\n",
    "$$\\epsilon(t) \\propto \\frac{1}{t}$$\n",
    "$$\\epsilon(t) = \\epsilon_0 \\alpha^t$$\n",
    "$$\\epsilon(t) = \\text{max}(\\epsilon_0-kt,\\epsilon_{min})$$\n",
    "$$\\epsilon(t) = \\frac{a}{\\text{log}(bt+c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsiCyIOSxL6H"
   },
   "source": [
    "# References\n",
    "- [Advancing Mobile A/B Testing with Bayesian Multi-Armed Bandit\n",
    "](https://splitmetrics.com/blog/multi-armed-bandit-in-a-b-testing/)\n",
    "- [Artificial Intelligence: Reinforcement Learning in Python](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Multi-Armed Bandits.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
