{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "There are a variety of problems we can find with the data for different variables in our datasets. Feature engineering involves transforming the data before sending it to an ML algorithm. This involves performing tasks such as filling in missing values within a variable or encoding categorical values or dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems Found in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "Missing data in a data set is the absence of values for certain observations within a variable. Missing data affects all supervised machine learning models. \n",
    "\n",
    "There are a variety of reasons why data can be missing:\n",
    "- Data is lost\n",
    "- Data is not stored properly\n",
    "- The value is undefined or can't exist (division where denominator is 0)\n",
    "- Survey data where user didn't answer a question\n",
    "\n",
    "When we have missing data we need to either remove the observation or provide some sort of default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels in Categorical Variables\n",
    "\n",
    "When the values of a categorical variable are strings rather than numbers, we have to transform them so that we can use them in our models.\n",
    "\n",
    "The unique set of values a categorical variable can take on are called labels or categories. The cardinality describes the number of labels a variable has.\n",
    "\n",
    "There are three main problems you find with categorical variables\n",
    "- High cardinality - High number of labels\n",
    "- Rare labels - infrequent categories\n",
    "- String data type - Categories aren't numeric\n",
    "\n",
    "Tree-based algorithms tend to overfit when you have variables with \"high cardinality\", i.e. a high number of labels. \n",
    "\n",
    "Rare labels can cause a problem because they are in so few observations in the overall dataset that there is a chance that they only end up in the training set or in the test set.\n",
    "\n",
    "Most categorical data, such as sex, year, or color are captured as strings or numbers (e.g. year) that don't have a specific numerical meaning. These variables needs to be *encoded*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Linear models assume that the data follows a Gaussian distribution. If the numerical variables in the data are skewed or not normal, we may have to apply a transformation.\n",
    "\n",
    "Other models like Support Vector Machines (SVMs) and Neural Networks do not make any variable assumptions, however a better spread of values over a larger range tends to improve the predictive performance of these algorithms.\n",
    "\n",
    "\n",
    "References:\n",
    "- [Feature Engineering-How to Transform Data to Better Fit The Gaussian Distribution-Data Science](https://www.youtube.com/watch?v=U_wKdCBC-w0&ab_channel=KrishNaik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Are there any unusual or unexpected values? Maybe they are extremely high, or extremely low compared to other observation values for the given variable.\n",
    "\n",
    "Outliers may affect certain machine learning models. For example, with a linear regression, outliers can easily change the slope of the regression line, especially in smaller datasets. The Adaboost algorithm is also extremely sensitive to outliers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Magnitude\n",
    "\n",
    "Most supervised machine learning algorithms are sensitive to the scale of the variables. \n",
    "\n",
    "Sensitive to feature scale:\n",
    "- Linear and Logistic Regression\n",
    "- Neural Networks\n",
    "- Support Vector Machines\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- K-Means Clustering\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "Tree-based ML models insensitive to feature scale:\n",
    "- Classification and Regression Trees\n",
    "- Random Forests\n",
    "- Gradient Boosted Trees\n",
    "\n",
    "Many of the training algorithms for linear models and neural networks can converge faster when the variables have a similar scale. So, we can use ***feature scaling*** to make all the variables have a similar scale.\n",
    "\n",
    "The distance-based algorithms such as KNN and K-Means Cluster are also sensitive to scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
