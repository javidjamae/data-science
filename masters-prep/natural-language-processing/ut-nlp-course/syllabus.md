# Natural Language Processing - Syllabus

Source: [Machine Learning](https://cdso.utexas.edu/msai) 

This course focuses on modern natural language processing using statistical methods and deep learning. Problems addressed include syntactic and semantic analysis of text as well as applications such as sentiment analysis, question answering, and machine translation. Machine learning concepts covered include binary and multiclass classification, sequence tagging, feedforward, recurrent, and self-attentive neural networks, and pre-training / transfer learning.

What You Will Learn
* Linguistics fundamentals: syntax, lexical and distributional semantics, compositional semantics
* Machine learning models for NLP: classifiers, sequence taggers, deep learning models
* Knowledge of how to apply ML techniques to real NLP tasks

Syllabus
* ML fundamentals, linear classification, sentiment analysis (1.5 weeks)
* Neural classification and word embeddings (1 week)
* RNNs, language modeling, and pre-training basics (1 week)
* Tagging with sequence models: Hidden Markov Models and Conditional Random Fields (1 week)
* Syntactic parsing: constituency and dependency parsing, models, and inference (1.5 weeks)
* Language modeling revisited (1 week)
* Question answering and semantics (1.5 weeks)
* Machine translation (1.5 weeks)
* BERT and modern pre-training (1 week)
* Applications: summarization, dialogue, etc. (1-1.5 weeks)

**Greg Durrett**
Assistant Professor, Computer Science