{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "97cd0b0c-7163-48e1-9bae-4c6b7a46e635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589b3e73-9145-4794-b72d-ec69a4f45fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length:  1115394\n",
      "----\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "----\n",
      "\n",
      "text[0:100] == text[:100] => True\n"
     ]
    }
   ],
   "source": [
    "# read the contents of the file into a variable called text\n",
    "with open( 'input.txt', 'r', encoding='utf-8' ) as f:\n",
    "    text = f.read()\n",
    "\n",
    "print( \"Text length: \", len(text))\n",
    "print( \"----\\n\" )\n",
    "print(text[0:100])\n",
    "print( \"----\\n\" )\n",
    "print( \"text[0:100] == text[:100] =>\", text[0:100] == text[:100] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f300c7ec-aedc-46b1-ae6c-1e152c63260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size:  65\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab: \", ''.join(chars))\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae27e64f-c40d-41ab-854c-3b24d0967c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3098202c-193e-4702-af67-884aace2790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode takes a string and returns a list of integers\n",
    "encode = lambda s: [ stoi[c] for c in s ]\n",
    "decode = lambda l: ''.join( itos[i] for i in l )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fef061b7-836b-4c09-ab55-3e05e648ea7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 39, 50, 50]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('ball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecda652f-1ca6-4882-a9b1-4537b9bf2d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ball'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([40, 39, 50, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fccf1eba-3d2b-4557-b673-9a3e8615bc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sentence'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('this is a sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcd5d7f5-5c93-4933-9208-099ff7ee6938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the entire text data set and store it in a tensor\n",
    "import torch\n",
    "data = torch.tensor( encode(text), dtype=torch.long )\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "498bd21e-c4d9-4835-b669-e28fa6afc338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(data[:62].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3831ca6-3546-4334-a4b6-84d6d82f5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# split the data up into train / validation\n",
    "n = int( 0.9 * len(data) )\n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]\n",
    "print( len(train) + len(validation) == len( data ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04c296a0-73a6-444c-bbbe-05b063f6aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "print( data[:block_size] )\n",
    "print( data[1:block_size+1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6852ea6-df3f-4361-a8f5-85d1ab724d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([18]) - Target: 47\n",
      "Input: tensor([18, 47]) - Target: 56\n",
      "Input: tensor([18, 47, 56]) - Target: 57\n",
      "Input: tensor([18, 47, 56, 57]) - Target: 58\n",
      "Input: tensor([18, 47, 56, 57, 58]) - Target: 1\n",
      "Input: tensor([18, 47, 56, 57, 58,  1]) - Target: 15\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15]) - Target: 47\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15, 47]) - Target: 58\n"
     ]
    }
   ],
   "source": [
    "x = data[:block_size]\n",
    "y = data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print( f\"Input: { context } - Target: { target }\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fb3d3d9-5a63-4e63-ab2e-e1ccdb7c8bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115386\n"
     ]
    }
   ],
   "source": [
    "print(len(data) - block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87c2b374-05fe-403f-b44f-053519251b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "(batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4b97ddd-c179-4c39-9bee-1e5d5fa2d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([978324, 638409, 104713,  75569])\n",
      "tensor(978324)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "tensor([12,  0,  0, 28, 39, 45, 43, 10])\n",
      "tensor([ 0,  0, 28, 39, 45, 43, 10,  0])\n"
     ]
    }
   ],
   "source": [
    "# Get 4 random block starting points\n",
    "ix_temp = torch.randint(len(data) - block_size, (batch_size,))\n",
    "print(ix_temp)\n",
    "print(ix_temp[0])\n",
    "print(train_data[0:10])\n",
    "print(train_data[ix_temp[0]:ix_temp[0]+block_size])\n",
    "print(train_data[ix_temp[0]+1:ix_temp[0]+block_size+1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "27ba97b8-353a-4fb1-be8a-fc56662ebe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[47, 58,  1, 57, 43, 43, 51, 57],\n",
      "        [43, 50,  1, 51, 43,  1, 58, 53],\n",
      "        [39, 50, 50,  1, 42, 43, 57, 54],\n",
      "        [ 1, 54, 56, 39, 63,  1, 63, 53]])\n",
      "tensor([[58,  1, 57, 43, 43, 51, 57,  1],\n",
      "        [50,  1, 51, 43,  1, 58, 53,  1],\n",
      "        [50, 50,  1, 42, 43, 57, 54, 39],\n",
      "        [54, 56, 39, 63,  1, 63, 53, 59]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    ix = torch.randint( len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack( [ data[i:i+block_size] for i in ix ] )\n",
    "    y = torch.stack( [ data[i+1:i+1+block_size] for i in ix ] )\n",
    "    return x, y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01294d86-54bc-49d3-a815-3ceb0dbdf940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([6]) - Target: 0\n",
      "Input: tensor([6, 0]) - Target: 21\n",
      "Input: tensor([ 6,  0, 21]) - Target: 44\n",
      "Input: tensor([ 6,  0, 21, 44]) - Target: 1\n",
      "Input: tensor([ 6,  0, 21, 44,  1]) - Target: 61\n",
      "Input: tensor([ 6,  0, 21, 44,  1, 61]) - Target: 43\n",
      "Input: tensor([ 6,  0, 21, 44,  1, 61, 43]) - Target: 1\n",
      "Input: tensor([ 6,  0, 21, 44,  1, 61, 43,  1]) - Target: 61\n",
      "Input: tensor([58]) - Target: 52\n",
      "Input: tensor([58, 52]) - Target: 43\n",
      "Input: tensor([58, 52, 43]) - Target: 57\n",
      "Input: tensor([58, 52, 43, 57]) - Target: 57\n",
      "Input: tensor([58, 52, 43, 57, 57]) - Target: 2\n",
      "Input: tensor([58, 52, 43, 57, 57,  2]) - Target: 1\n",
      "Input: tensor([58, 52, 43, 57, 57,  2,  1]) - Target: 57\n",
      "Input: tensor([58, 52, 43, 57, 57,  2,  1, 57]) - Target: 43\n",
      "Input: tensor([1]) - Target: 59\n",
      "Input: tensor([ 1, 59]) - Target: 52\n",
      "Input: tensor([ 1, 59, 52]) - Target: 39\n",
      "Input: tensor([ 1, 59, 52, 39]) - Target: 41\n",
      "Input: tensor([ 1, 59, 52, 39, 41]) - Target: 46\n",
      "Input: tensor([ 1, 59, 52, 39, 41, 46]) - Target: 47\n",
      "Input: tensor([ 1, 59, 52, 39, 41, 46, 47]) - Target: 52\n",
      "Input: tensor([ 1, 59, 52, 39, 41, 46, 47, 52]) - Target: 45\n",
      "Input: tensor([43]) - Target: 42\n",
      "Input: tensor([43, 42]) - Target: 50\n",
      "Input: tensor([43, 42, 50]) - Target: 39\n",
      "Input: tensor([43, 42, 50, 39]) - Target: 56\n",
      "Input: tensor([43, 42, 50, 39, 56]) - Target: 6\n",
      "Input: tensor([43, 42, 50, 39, 56,  6]) - Target: 1\n",
      "Input: tensor([43, 42, 50, 39, 56,  6,  1]) - Target: 50\n",
      "Input: tensor([43, 42, 50, 39, 56,  6,  1, 50]) - Target: 43\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # Batch dimension\n",
    "    for t in range(block_size): # Time dimension\n",
    "        context = xb[b][:t+1]\n",
    "        target = yb[b][t]\n",
    "        print( f\"Input: { context } - Target: { target }\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6416f1-5c88-4932-ae9e-f107b2d7b4be",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "43f63157-2be3-4972-a019-65bb3060336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx is the inputs\n",
    "        # targets is the outputs\n",
    "        # B -> Batch   -> Batch Size (4)\n",
    "        # T -> Time    -> Block Size (8)\n",
    "        # C -> Channel -> Vocab Size (65)\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e7a824bc-eece-4368-be41-b981aae56d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "tensor([-0.2060,  1.5973,  0.1185, -1.2549,  1.3024,  0.4760, -0.8871,  1.3709,\n",
      "        -1.9473, -0.8017, -1.3055, -0.4910,  0.4430,  0.2178, -0.3297, -0.0192,\n",
      "         0.9225,  0.9187,  0.2998,  0.6106,  0.7791,  0.1237,  1.8620,  1.7080,\n",
      "        -1.6045,  0.3338, -2.0513,  0.5923,  0.4880, -1.4055, -0.6686, -0.4831,\n",
      "        -0.2298,  0.9043,  0.7631, -0.1606,  0.9156, -0.6908, -0.3065, -1.1809,\n",
      "         0.8175, -2.0392,  0.1558, -0.2996, -0.5391, -0.3657,  0.8282, -0.4826,\n",
      "         1.8330,  0.3421,  0.2154, -0.1029, -0.0946,  0.0070,  0.1484, -0.5403,\n",
      "        -1.9312, -0.7858, -0.6731, -0.0901,  0.2598, -0.5349,  0.5812, -0.5356,\n",
      "        -1.7944], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.2060,  1.5973,  0.1185, -1.2549,  1.3024,  0.4760, -0.8871,  1.3709,\n",
      "         -1.9473, -0.8017, -1.3055, -0.4910,  0.4430,  0.2178, -0.3297, -0.0192,\n",
      "          0.9225,  0.9187,  0.2998,  0.6106,  0.7791,  0.1237,  1.8620,  1.7080,\n",
      "         -1.6045,  0.3338, -2.0513,  0.5923,  0.4880, -1.4055, -0.6686, -0.4831,\n",
      "         -0.2298,  0.9043,  0.7631, -0.1606,  0.9156, -0.6908, -0.3065, -1.1809,\n",
      "          0.8175, -2.0392,  0.1558, -0.2996, -0.5391, -0.3657,  0.8282, -0.4826,\n",
      "          1.8330,  0.3421,  0.2154, -0.1029, -0.0946,  0.0070,  0.1484, -0.5403,\n",
      "         -1.9312, -0.7858, -0.6731, -0.0901,  0.2598, -0.5349,  0.5812, -0.5356,\n",
      "         -1.7944]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print( decode([15]) ) # The letter C\n",
    "\n",
    "print( m.token_embedding_table.weight[15] ) # the weights for embedding 15 (C)\n",
    "\n",
    "some_x = torch.tensor([ 15 ])\n",
    "# Right now, y isn't doing anything in our forward function so we can change it with no impact\n",
    "some_y = torch.tensor([ 22 ]) \n",
    "out = m( some_x, some_y ) \n",
    "print( out )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "145c6894-082d-496e-8e98-670a23adfc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yourbM\n",
      "3LUZurpqB&W-Z XiS\n",
      "\n",
      "iy!FvS-:\n",
      "jbcGpE,GIoyeS?WsU'KWKwuK:RDTsSxO.LuW;.AHoV3XiLo.KLmuFkdRbzSJaiSMK:-gyKQIE&WqFbljnhycvfvK:Uo\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # targets is the outputs, also (B, T)\n",
    "        # B -> Batch   -> Batch Size (4)\n",
    "        # T -> Time    -> Block Size (8)\n",
    "        # C -> Channel -> Vocab Size (65)\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # We have a multi-dimensional input (B, T, C) so this won't work\n",
    "            #  the F.corss_entropy wants the C to be the second dimension, so (B, C, T)\n",
    "            #loss = F.cross_entropy(logits, targets)\n",
    "            # So, we have to reshape our logits\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Stretch it out\n",
    "            reshaped_targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy( logits, reshaped_targets )\n",
    "\n",
    "            # Seems like you could also do this\n",
    "            #reshaped_logits = logits.permute(0, 2, 1) \n",
    "            #loss = F.cross_entropy( reshaped_logits, targets )\n",
    "            #print(loss)\n",
    "\n",
    "        # Not the best code, returns different logits dimensions \n",
    "        #    depending on whether the targets were provided. If provided\n",
    "        #    it returns the reshaped dimensions, otherwise, the original.\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context (i.e. the encoded string(s) we're trying to generate additional text for)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            #   i.e. grab the logits representing the last character in the string represented by idx)\n",
    "            #   i.e. plucks out the last element of the time dimension\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probabilities, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            #   dim=1 means to use the T (time) dimension, since idx is (B, T)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# logits, loss = m(xb, yb)\n",
    "# print(logits.shape)\n",
    "\n",
    "# torch.zeros((1, 1), dtype=torch.long)\n",
    "# torch.tensor([encode('your')])\n",
    "\n",
    "out = model.generate( torch.tensor([encode('your')]), 20 )[0].tolist()\n",
    "print( decode( out ) )\n",
    "# m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8f734-a8f6-49ab-bff0-ffd956865da5",
   "metadata": {},
   "source": [
    "## Training the Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c43d197d-551e-45a1-aa42-800e8b456166",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d6cfb344-6fe5-4f45-8e2f-2216e0485c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4422693252563477\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "\n",
    "for i in range(max_steps):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zero-out gradients from previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    # This will update all of our parameters, magically!\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bf6ed357-0b10-4cdd-a007-83918998319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yourd ke s bith main JUS\n",
      "\n",
      "\n",
      "S:\n",
      "ONRIULO:\n",
      "Nequ hickn araroug,\n",
      "\n",
      "\n",
      "HAnor the frein se r s\n",
      "\n",
      "LLe bethitcais icheldee sp-eende chth lec\n"
     ]
    }
   ],
   "source": [
    "out = model.generate( torch.tensor([encode('your')]), 20 )[0].tolist()\n",
    "print( decode( out ) )\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a3d19-ad94-4225-8982-f8a0007e3fe3",
   "metadata": {},
   "source": [
    "# Math Trick for self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749ee4d-d4c4-45a7-a518-2d02fbd20971",
   "metadata": {},
   "source": [
    "\n",
    "We want tokens to only communicate with past tokens. One way is to take the averages of all the previous tokens (even though we lose the positional information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5c2e01c0-dc4f-4609-ba26-d8704ea60526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537bcbf-b5dd-4d17-85a4-3db66f63c365",
   "metadata": {},
   "source": [
    "### Inefficient approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ffa9e04c-9279-4901-8f75-93678efeb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # x bag-of-words (used when averaging)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "822d91b6-40b6-4de6-b6b5-a473cc319479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "80859d88-eeb2-4c44-900e-a758fb96e9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0acdf0f-4f28-4c3a-b1b4-0640f5a527a9",
   "metadata": {},
   "source": [
    "The first row is the same, but we see subsequent rows are the average of all previous rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d9ab66ee-59fc-4227-a8ee-48b36a6e6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0341,  0.1332])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be the same as what we see in the last row\n",
    "x[0].mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9896b1f-4384-4d93-9b3b-3807fe009cb1",
   "metadata": {},
   "source": [
    "### Efficient Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "2052821d-90c6-4665-9db4-0d3add80fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "dddc738d-34cf-4c04-b3ee-98df3964e1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns lower triangular portion of matrix\n",
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "fe3ee8d4-0556-41e8-8e1f-11f353c7af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.tril(torch.ones(3,3))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b7254a20-5b55-460d-be69-4b580ca2d634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8e5043e3-9951-4fcc-98b4-7a5301cfca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 2., 1.])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(temp, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e94543b9-f044-4d0a-a7f7-a625027b1be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(temp, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "5599d782-b046-4a6f-9e9b-5ca185fc80be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(temp, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "55a5a97a-8882-4414-af55-a71cdb34b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "temp = temp / torch.sum(temp, dim=1, keepdim=True)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "43a26766-ef49-4862-8095-da7886c6ae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a76adf-92e2-48cb-a14a-95db701dadb3",
   "metadata": {},
   "source": [
    "So..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "5f55049e-795f-429e-888c-77a2c3b2b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril( torch.ones(T,T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c6d86761-228b-4ac0-b625-76e9d107243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei / wei.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "eb229ea7-51db-4af7-aa34-4d5421df176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6ee9d133-d534-4503-bd22-bbffe4cc0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is from way at the top of this section\n",
    "\n",
    "# (T, T) @ (B, T, C) -> \n",
    "# (B, T, T) @ (B, T, C) -> \n",
    "# (B, T, C)\n",
    "xbow2 = wei @ x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "98e09d7b-f919-445c-8326-583fa4556d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare inefficient method to efficient method\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5f04b-7c43-4c30-85b6-4d0626769f3f",
   "metadata": {},
   "source": [
    "### One More Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "1f47fc21-8fae-449f-b5fc-51a543b99d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril( torch.ones( T, T) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "47953a4f-7977-4683-8970-b47b3242611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.zeros((T,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "645bef43-2468-4032-8f1f-203a644383c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "wei = wei.masked_fill( tril == 0, float('-inf'))\n",
    "print( wei )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "adb6c870-eb59-4eb1-8c24-be9390913286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "cacc87e4-5409-48d4-b992-1ff8f2294d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril( torch.ones( T, T) )\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280218f-8744-44ab-8268-95fb2e07e777",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7ad95-b7ca-4413-b2b4-0c103e20f7fb",
   "metadata": {},
   "source": [
    "The current code just takes a simple average of current and past tokens using the lower triangular structure. But, we don't necessarily want this weighting to be uniform because some tokens will find other tokens more intersting and we may want them to have a higher affinity. A vowel may want constanants from the past, in a data-dependent way. \n",
    "\n",
    "This is what self-attention solves. \n",
    "\n",
    "Every single node/token at each position will emit two vectors: query and key.\n",
    "\n",
    "Query = What am I looking for\n",
    "Key = What do I contain\n",
    "\n",
    "Affinities are the dot products between the queries and the keys. So, for a given node/token, you can take its query and dot product it with all the other keys for all the other nodes/tokens. That resulting dot product becomes the new \"wei\" (weights??).\n",
    "\n",
    "If the key/query are aligned, they will interact in a high amount, and I'll learn more about that token than other tokens in the sequence.\n",
    "\n",
    "We're now going to implement a single \"head\" of self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ca957035-1d41-4629-993e-498918558fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([4, 8, 32])\n",
      "k shape:  torch.Size([4, 8, 16])\n",
      "q shape:  torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "print(\"x shape: \", x.shape)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "# A linear layer will transform from the in_features to the out_features\n",
    "head_size = 16\n",
    "key = nn.Linear(in_features=C, out_features=head_size, bias=False)\n",
    "query = nn.Linear(in_features=C, out_features=head_size, bias=False) \n",
    "value = nn.Linear(in_features=C, out_features=head_size, bias=False)\n",
    "\n",
    "# (B, T, C) -> (B, T, head_size)\n",
    "# In parallel, this produces a key/query for every token\n",
    "#     so no communication has happened (yet)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "# wei is now the affinities between the keys/queries\n",
    "#  to do the multiplication, we have to transpose the last two dimensions\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "# for every batch we now have a (T, T) matrix representing the affinities\n",
    "\n",
    "print( \"k shape: \", k.shape)\n",
    "print( \"q shape: \", q.shape)\n",
    "\n",
    "tril = torch.tril( torch.ones( T, T) )\n",
    "# No longer using zeros\n",
    "#wei = torch.zeros((T,T))\n",
    "\n",
    "# This is used in a \"decoder\" where you're using an auto-regressive approach\n",
    "#   so we don't talk to the future nodes. But if we wanted an \"encoder\", for\n",
    "#   something like sentiment analysis for example, then we'd just need to \n",
    "#   delete this line so that nodes can all talk to each other.\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "#out = wei @ x\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "\n",
    "out.shape\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13048b74-074f-4f6c-a1e9-1d1ea14ec49d",
   "metadata": {},
   "source": [
    "This now tells us how much of the data to aggregate from tokens in the past.\n",
    "\n",
    "x is thought of as private information for each token (the embedding value??). \n",
    "k is what each token has\n",
    "q is what each token is looking for\n",
    "v is what is communicated based on the affinities between different tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca438bec-589a-4e62-8629-3bba8cfb66b1",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "c5ced12e-deee-4719-a930-82b685925b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei_naive = q @ k.transpose(-2, -1)\n",
    "\n",
    "# During initialization we need wei to be diffuse\n",
    "wei_scaled  = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "24bcd398-ac9c-48e4-8772-6318774ffeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0966)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "f74b07c8-462e-4bf2-8588-b73ff6271f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9416)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "bbf96c26-1bb2-4ab5-a866-74e164afaf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.1036)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_naive.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c51f5e9b-fa0c-46c3-ae7c-183a7641e7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0065)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bfd96-45d4-425b-a03d-1e9a98bb1699",
   "metadata": {},
   "source": [
    "During initialization we want our values to be fairly diffuse, because if they're too big then they will give too much bias to the biggest term. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "54cf87ca-01f7-4aab-aa8f-83348866a6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "70b61001-2206-4162-b667-aad95f7f0593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8f091-a720-4ea9-8459-d5a2d4a52cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
