{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f71d420-ab51-4406-89c8-b328fdc002cb",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "References:\n",
    "* [Building a neural network from scratch](https://www.youtube.com/watch?v=w8yWXqWQYmU)\n",
    "* [Understanding the math behind neural networks by building one from scratch (no TF/Keras, just numpy)](https://www.samsonzhang.com/2020/11/24/understanding-the-math-behind-neural-networks-by-building-one-from-scratch-no-tf-keras-just-numpy)\n",
    "* [MNIST database](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "\n",
    "\n",
    "I'm going to build a neural network from the ground up using Python and basic libraries, without any ML libraries. I'm going to create a neural network to do handwriting recognition against the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "Based on the [MNIST docs](http://yann.lecun.com/exdb/mnist/):\n",
    "```\n",
    "Each image in the database is 28x28 pixels with a grayscale value for each pixel, ranging between 0 and 255. Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black). \n",
    "```\n",
    "\n",
    "We're going to have $m$ training images.\n",
    "\n",
    "Call the examples $E$ (a matrix), we would have a matrix where each row has 784 columns.\n",
    "\n",
    "$\n",
    "\\begin{array}{@{}c@{}}\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 1 & ... & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & ... & 1 & 0 & 0 \\\\\n",
    "    &&& ... &&& \\\\\n",
    "    0 & 0 & 0 & ... & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\begin{array}{@{}l@{}}\n",
    "    <- X^{(1)} \\\\\n",
    "    <- X^{(2)} \\\\\n",
    "    \\\\\n",
    "    <- X^{(m)} \\\\\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "We will be transposing this to be $X = E^T$, where there are a fixed 784 rows, and a new column for each example. So, this will be a matrix of size $784 \\times m$:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    | & | & | & | \\\\\n",
    "    | & | & | & | \\\\\n",
    "    | & | & | & | \\\\\n",
    "    X^{(1)} & X^{(2)} & ... & X^{(m)} \\\\\n",
    "    | & | & | & | \\\\\n",
    "    | & | & | & | \\\\\n",
    "    | & | & | & | \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "We're going to create a Neural Network with an input layer (784 nodes), one 10-node hidden layers, and a 10-node output layer representing the digits 0 to 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980cbb43-e8a6-4de9-b7fa-fe5adfcd852f",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "We'll call our input $A^{[0]}$, which is just equal to the tranformed matrix with our example data $X$. Therefore, the dimensions of this matrix will be $784 \\times m$.\n",
    "\n",
    "$\n",
    "A^{[0]} = X\n",
    "$\n",
    "\n",
    "$Z^{[1]}$ is the *unactivated* first layer. We're going to apply a weight and a bias to get it.\n",
    "\n",
    "$\n",
    "Z^{[1]} = W^{[1]} A^{[0]} + b^{[1]}\n",
    "$\n",
    "\n",
    "The dimensions of the matrices involved in calculating $Z^{[1]}$ are as follows:\n",
    "\n",
    "$\n",
    "\\begin{array}{@{}ll}\n",
    "    Z^{[1]} \\text{ is } 10 \\times m \\\\\n",
    "    W^{[1]} \\text{ is } 10 \\times 784 \\\\\n",
    "    A^{[0]} \\text{ is } 784 \\times m \\\\\n",
    "    b^{[1]} \\text{ is } 10 \\times 1 \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "Now we need to apply an activation function. We're going to use ReLU (Rectified Linear Unit). If we didn't apply an activation function, each node in the hidden layer would just be a linear combination of the nodes from the previous layer. If you don't apply an activation function, you're effectively just doing a linear regression, but by using a non-linear activation function you add non-linearity to the transformation, which allows more complexity in the solution space. ReLU allows it to be linear instead of using a sigmoid or something else **[research this more to understand it better]**.\n",
    "\n",
    "$ReLU( x ) = x \\text{ if } x > 0\\text{; } 0 \\text{ if } x <= 0$\n",
    "\n",
    "We'll call our ReLU-activated output of our input layer $A^{[1]}$\n",
    "\n",
    "$A^{[1]} \\text{ is } 10 \\times m$\n",
    "\n",
    "$A^{[1]} = g( Z^{[1]} ) = ReLU( Z^{[1]} )$\n",
    "\n",
    "Our unactivated output layer has the following dimensions:\n",
    "\n",
    "$\n",
    "\\begin{array}{@{}ll}\n",
    "    Z^{[2]} \\text{ is } 10 \\times m \\\\\n",
    "    W^{[2]} \\text{ is } 10 \\times 10 \\\\\n",
    "    A^{[1]} \\text{ is } 10 \\times m \\\\\n",
    "    b^{[2]} \\text{ is } 10 \\times 1 \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "The unactivated output for the output layer is:\n",
    "\n",
    "$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea73d67-3500-43e7-a82f-7245380b33fe",
   "metadata": {},
   "source": [
    "# Softmax explained\n",
    "\n",
    "To get the activated output of the layer we apply a ***softmax*** function to get the activated output. Softmax is a function that gives you a percentage normalization by taking each node in the output layer and dividing it by the sum of all of the nodes in the output layer. \n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function) we know the softmax function works like this:\n",
    "\n",
    "$\n",
    "\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$\n",
    "\n",
    "> ℹ️ Note\n",
    ">\n",
    "> Don't confuse any of the above variables such as $z$, $i$, $j$ or $K$ with anything we're referencing in this problem. These are just the generic definition of the function as per the Wikipedia page for softmax. More specifically, in the context of the softmax function and its derivative, $i$ and $j$ are indices over the classes. We compute the softmax function for each class $j$, and when computing the derivative of this function, we need to consider two cases, $i = j$ and $i ≠ j$. Separately, as you'll see below during backpropagation within the context of the activated output layer matrix (e.g., $A^{[2]}$), $i$ is the class index and $j$ is the example index. In that case, we're computing the loss function for each example and each class, and when we differentiate the loss with respect to the activations, and $i$ and $j$ are used as indices in this matrix.\n",
    "\n",
    "\n",
    "The softmax probability vector for the input vector $z$ is a normalized vector where each entry is the probability of the given input value. \n",
    "\n",
    "So for example, if we have the following input vector:\n",
    "$$z = \\begin{bmatrix} \n",
    "    2.3 \\\n",
    "    -1.7 \\\n",
    "    5.8 \\\n",
    "    0.9 \\\n",
    "    -3.2 \\\n",
    "    7.1 \\\n",
    "    4.6 \\\n",
    "    3.4 \\\n",
    "    -4.8 \\\n",
    "    6.5 \\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\text{softmax}(z) = \\begin{bmatrix} \n",
    "    0.005 \\\n",
    "    0.001 \\\n",
    "    0.184 \\\n",
    "    0.002 \\\n",
    "    0.000 \\\n",
    "    0.801 \\\n",
    "    0.008 \\\n",
    "    0.004 \\\n",
    "    0.000 \\\n",
    "    0.016 \\\n",
    "\\end{bmatrix}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da2fe3-2703-4ad5-a5a2-4b53a1984db5",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "Now that we have our unactivated output layer matrix $Z^{[2]}$, and we know how softmax works, we can use softmax to get our activated output layer probabability distribution. \n",
    "\n",
    "Remember, $Z^{[2]} \\text{ is } 10 \\times m$ so the dimensions of $A^{[2]} \\text{ are also } 10 \\times m$.\n",
    "\n",
    "$A^{[2]} = softmax( Z^{[2]} )$\n",
    "\n",
    "So for example, if $Z^{[2]}$ had the following for one of its examples:\n",
    "$$Z^{[2]} = \\begin{bmatrix} \n",
    "    2.3 & ... \\\\\n",
    "    -1.7 & ... \\\\\n",
    "    5.8 & ... \\\\\n",
    "    0.9 & ... \\\\\n",
    "    -3.2 & ... \\\\\n",
    "    7.1 & ... \\\\\n",
    "    4.6 & ... \\\\\n",
    "    3.4 & ... \\\\\n",
    "    -4.8 & ... \\\\\n",
    "    6.5 & ... \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then applying that as the input of the softmax function would result in:\n",
    "$$A^{[2]} = \\text{softmax}(Z^{[2]}) = \\begin{bmatrix} \n",
    "    0.005 & ... \\\\\n",
    "    0.001 & ... \\\\\n",
    "    0.184 & ... \\\\\n",
    "    0.002 & ... \\\\\n",
    "    0.000 & ... \\\\\n",
    "    0.801 & ... \\\\\n",
    "    0.008 & ... \\\\\n",
    "    0.004 & ... \\\\\n",
    "    0.000 & ... \\\\\n",
    "    0.016 & ... \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Each $A^{[2]}_{i,j}$ value represents the probability that the $j$ example is of class $i$.\n",
    "\n",
    "If we want to get a \"best guess\" answer, then we can select the highest probability answer and return that as the selected answer. To do this, we can use a ***one-hot encoding*** function. \n",
    "\n",
    "Let's say we have a prediction vector for a single example as $\\hat{y}$ as follows:\n",
    "\n",
    "$$\\hat{y} = \\begin{bmatrix} \n",
    "    0.005 \\\n",
    "    0.001 \\\n",
    "    0.184 \\\n",
    "    0.002 \\\n",
    "    0.000 \\\n",
    "    0.801 \\\n",
    "    0.008 \\\n",
    "    0.004 \\\n",
    "    0.000 \\\n",
    "    0.016 \\\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "Then we can define $y$ as the one-hot encoding of the correct label for the training example. If the label for a training example is 5 (because it has the highest probability as in the example above), then the one-hot encoded vector of $y$ would look like this:\n",
    "\n",
    "$$y = \\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}$$\n",
    "\n",
    "This tells us that given the image input values for this example, the classifier has selected 5 as the best guess on what digit this image is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c62262-7de3-40d1-a97f-e4e883244e49",
   "metadata": {},
   "source": [
    "# Loss and Gradient Decent Explained\n",
    "\n",
    "Backpropagation is the method of taking the loss of the output and using it to calculate changes that we should make to the weights and biases of the preceding layers to move us closer to a more accurate output. Each example will nudge us a little closer to a better solution. The approach we take is to use calculus to calculate the derivative of each component that contributes to the output and adjust accordingly. This approach is known as ***gradient decent***.\n",
    "\n",
    "Because we're using softmax, we want to use a ***cross-entropy loss function***. We can calculate the loss using the following function:\n",
    "$$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Here, $\\hat{y}$ is our prediction vector and $y$ is the one-hot encoding of the correct label from the examples in our training data. Because the one-hot vector is $0$ for all entries except for the one with the correct prediction, the loss will just be the $-\\log()$ of the probability associated with the correct prediction. \n",
    "\n",
    "So, if\n",
    "$$\\hat{y} = \\begin{bmatrix} 0.005 \\ 0.001 \\ 0.184 \\ 0.002 \\ 0.000 \\ 0.801 \\ 0.008 \\ 0.004 \\ 0.000 \\ 0.016 \\ \\end{bmatrix}$$ \n",
    "and\n",
    "$$y = \\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}$$\n",
    "then \n",
    "$$J(\\hat{y}, y) = -\\log(\\hat{y}_i) = -\\log(0.801) = 0.09636748392 $$\n",
    "\n",
    "The closer the probability is to $1$ the closer the loss will be to $0$.\n",
    "\n",
    "Now that we know what loss is, we need to work backwards through our network to find out what portion of the loss is composed by each of the weights and biases. \n",
    "\n",
    "$$\n",
    "\\begin{array}{@{}ll}\n",
    "W^{[1]} := W^{[1]} - \\alpha \\frac{\\delta J}{\\delta W^{[1]}} \\\\\n",
    "b^{[1]} := b^{[1]} - \\alpha \\frac{\\delta J}{\\delta b^{[1]}} \\\\\n",
    "W^{[2]} := W^{[2]} - \\alpha \\frac{\\delta J}{\\delta W^{[2]}} \\\\\n",
    "b^{[2]} := b^{[2]} - \\alpha \\frac{\\delta J}{\\delta b^{[2]}} \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc18e75-0b98-4ea9-a21f-8fcb1442a731",
   "metadata": {},
   "source": [
    "# Backpropagation - Derivative of Loss\n",
    "\n",
    "In order to calculate the weights and biases for the previous layers, we have to work our way backwards. We have a loss function that is applied to an activated output layer (softmax) that is applied to an unactivated output layer.\n",
    "\n",
    "So, how can we calculate the derivative of the loss with respect to the unactivated output layer?\n",
    "\n",
    "Remember our loss function is:\n",
    "$$J(\\hat{y}, y) = -\\sum_{i=1}^{c} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Let's focus on a single example vector rather than the whole matrix to calculate the derivative. \n",
    "\n",
    "For our case, $\\hat{y}$ is our prediction vector $A^{[2]}$ and $y$ is our one-hot encoded vector for a single example, which we can call $Y$. So:\n",
    "$$\n",
    "\\hat{y} = A^{[2]} \\text{, } y = Y \n",
    "$$\n",
    "\n",
    "> ℹ️ Note\n",
    ">\n",
    "> In this section we're using $A^{[2]}$ and $Y$ to represent a single example vector and $A^{[2]}_i$ and $Y_i$ to refer to a single entry in that vector.\n",
    "\n",
    "Now we can plug those in:\n",
    "$$J(\\hat{y}, y) = J( A^{[2]}, Y ) = -\\sum_{i=1}^{c} Y_i \\log(A^{[2]}_i)$$\n",
    "\n",
    "Now we want to calculate:\n",
    "$$\\frac{\\partial J}{\\partial A^{[2]}_i}$$\n",
    "\n",
    "Let's tackle the derivative inside of the summation, then apply the derivative of the summation. We'll bring the negative into the summation. We know that the derivative of $log(x)$ is $1/x$. Differentiating inside the summation we have:\n",
    "$$ \\frac{\\partial (-Y_i \\log(A^{[2]}_i))}{\\partial A^{[2]}_i} = -\\frac{Y_i}{A^{[2]}_i}$$\n",
    "\n",
    "Next, we know that:\n",
    "$A^{[2]} = \\text{softmax}( Z^{[2]} )$\n",
    "\n",
    "The derivative of the softmax function depends on whether $i=j$ or $i≠j$. In the $i=j$ case, the derivative simplifies to $\\text{softmax}(x_i) * (1 - \\text{softmax}(x_i))$ and in the $i≠j$ case, it is $-\\text{softmax}(x_i) * \\text{softmax}(x_j)$.\n",
    "\n",
    "In our case, that means the derivative of the softmax is (for the $i=j$ case):\n",
    "$$\\frac{\\partial A^{[2]}_i}{\\partial Z^{[2]}_i} = \\text{softmax}'( Z^{[2]}_i ) = \\text{softmax}(Z^{[2]}_i) * (1 - \\text{softmax}(Z^{[2]}_i)) = A^{[2]}_i * (1 - A^{[2]}_i)$$\n",
    "\n",
    "Using the chain rule, we can calculate the loss for $\\frac{\\partial J}{\\partial Z^{[2]}_i}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial Z^{[2]}_i} = \\frac{\\partial J}{\\partial A^{[2]}_i} * \\frac{\\partial A^{[2]}_i}{\\partial Z^{[2]}_i} = -Y_i (1 - A^{[2]}_i)\n",
    "$$\n",
    "\n",
    "Because $Y$ is **one-hot** encoded, this simplifies. When considering the derivative of the softmax cross entropy loss with respect to $Z^{[2]}_j$, two cases arise, one when $i$ equals $j$, and one when $i$ doesn't equal $j$. In the first case, the derivative is:\n",
    "$$Y_j (1 - A^{[2]}_j) \\text{ when } i = j$$\n",
    "\n",
    "And in the second case, it is:\n",
    "$$-Y_i A^{[2]}_j$$. However, summing over all $i$, only the terms where $i$ equals $j$ survive in the $Y$ vector due to the nature of one-hot encoding. This gives us the difference $A^{[2]}_j - Y_j$, or in vector form $A^{[2]} - Y$.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial Z^{[2]}} = A^{[2]} - Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc80555-71b7-4cd8-80f9-a2251c74c21e",
   "metadata": {},
   "source": [
    "# Backpropagation - Calculating Weights and Biases\n",
    "\n",
    "Once you have the derivative of the loss function with respect to the linear outputs (in this case $ \\frac{\\partial J}{\\partial Z^{[2]}} $), the next steps in backpropagation involve calculating the gradients with respect to the weights, biases, and activations of previous layers. Here's the procedure:\n",
    "\n",
    "1. **Calculate the gradient with respect to the weights $W^{[2]}$ of the current layer:**\n",
    "   \n",
    "   Using the chain rule, the gradient of the loss with respect to the weights of the output layer can be calculated as:\n",
    "   $$ \\frac{\\partial J}{\\partial W^{[2]}} = \\frac{\\partial J}{\\partial Z^{[2]}} \\times \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} $$\n",
    "   \n",
    "   Given that $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$, the gradient becomes:\n",
    "   $$ \\frac{\\partial J}{\\partial W^{[2]}} = dZ^{[2]} A^{[1]T} $$\n",
    "\n",
    "2. **Calculate the gradient with respect to the biases $b^{[2]}$ of the current layer:**\n",
    "   \n",
    "   This can be calculated as:\n",
    "   $$ \\frac{\\partial J}{\\partial b^{[2]}} = \\frac{\\partial J}{\\partial Z^{[2]}} \\times \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} $$\n",
    "   \n",
    "   Given the way biases work (they get added element-wise to each example in the batch), the gradient with respect to the biases becomes a sum across all examples:\n",
    "   $$ \\frac{\\partial J}{\\partial b^{[2]}} = \\sum_{i} dZ^{[2](i)} $$\n",
    "   where the sum is over all examples in the batch.\n",
    "\n",
    "3. **Calculate the gradient with respect to the activations of the previous layer $A^{[1]}$:**\n",
    "\n",
    "   This can be calculated using:\n",
    "   $$ \\frac{\\partial J}{\\partial A^{[1]}} = \\frac{\\partial J}{\\partial Z^{[2]}} \\times \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} $$\n",
    "   \n",
    "   Given $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$, the gradient becomes:\n",
    "   $$ \\frac{\\partial J}{\\partial A^{[1]}} = W^{[2]T} dZ^{[2]} $$\n",
    "\n",
    "4. **Move to the Previous Layer:**\n",
    "   \n",
    "   To compute the gradients for parameters in the previous layer (i.e., layer 1 in this case), you'd repeat the process, using the activation function of that layer and its derivative. Let's say layer 1 uses the ReLU activation function, then you would need to compute:\n",
    "   $$ dZ^{[1]} = \\frac{\\partial J}{\\partial A^{[1]}} * g'^{[1]}(Z^{[1]}) $$\n",
    "   where $g'^{[1]}(Z^{[1]})$ is the derivative of the ReLU function applied element-wise to $Z^{[1]}$.\n",
    "\n",
    "5. **Update the Weights and Biases:**\n",
    "\n",
    "   After calculating all the required gradients, use them to update the weights and biases. This is typically done using an optimization algorithm like Gradient Descent or one of its variants:\n",
    "   $$ W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}} $$\n",
    "   $$ b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial J}{\\partial b^{[l]}} $$\n",
    "   where $\\alpha$ is the learning rate and $l$ denotes the layer.\n",
    "\n",
    "6. **Iterate:**\n",
    "\n",
    "   The above steps are iteratively performed for multiple epochs or until a certain convergence criterion is met, leading to the model's weights and biases being updated to minimize the loss function.\n",
    "\n",
    "Note: The actual backpropagation calculations are dependent on the specific architecture and activation functions used in the neural network. This summary provides a general idea and is based on the context provided previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ee7e20-4767-4186-8c70-569e7da21d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using tensorflow to get the mnist data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8666844d-90c1-4b0c-8b69-143b1e7c8f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  0  1  2  3  4  5  6  7  8  ...  774  775  776  777  778  779  780  \\\n",
      "0      5  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      4  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3      1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      9  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "   781  782  783  \n",
      "0    0    0    0  \n",
      "1    0    0    0  \n",
      "2    0    0    0  \n",
      "3    0    0    0  \n",
      "4    0    0    0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "data = pd.DataFrame(train_images.reshape(train_images.shape[0], -1))\n",
    "data['label'] = train_labels\n",
    "\n",
    "# Get the column names of the DataFrame\n",
    "columns = data.columns.tolist()\n",
    "\n",
    "# Move the 'label' column to the first position\n",
    "label_index = columns.index('label')\n",
    "columns = ['label'] + columns[:label_index] + columns[label_index+1:]\n",
    "\n",
    "# Reorder the DataFrame with the new column order\n",
    "data = data[columns]\n",
    "\n",
    "# Display the DataFrame with 'label' as the first column\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5762e2b9-5565-44c4-9d3c-f4d624f1c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# X is the data\n",
    "# Y is the labels\n",
    "\n",
    "data_dev = data[ 0 : 1000 ].T\n",
    "Y_dev = data_dev[ 0 ]\n",
    "X_dev = data_dev[ 1 : n ]\n",
    "X_dev = X_dev / 255\n",
    "\n",
    "data_train = data[ 1000 : m ].T\n",
    "Y_train = data_train[ 0 ]\n",
    "X_train = data_train[ 1 : n ]\n",
    "X_train = X_train / 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0df66bbc-5c80-4cde-8d79-24d4f1e67786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (training data):  (784, 59000)\n",
      "X_dev (dev data):  (784, 1000)\n",
      "Y_train (training labels):  (59000,)\n",
      "Y_dev (dev labels):  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the training and dev data\n",
    "print( \"X_train (training data): \", X_train.shape )\n",
    "print( \"X_dev (dev data): \", X_dev.shape )\n",
    "\n",
    "#print the shape of the label data\n",
    "print( \"Y_train (training labels): \", Y_train.shape )\n",
    "print( \"Y_dev (dev labels): \", Y_dev.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64c9195a-e0ca-4d89-ae7b-585bcf20de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # W1 is 10x784\n",
    "    W1 = np.random.rand( 10, 784 ) - 0.5\n",
    "    # b1 is 10x1\n",
    "    b1 = np.random.rand( 10, 1 ) - 0.5\n",
    "    # W2 is 10x784\n",
    "    W2 = np.random.rand( 10, 10 ) - 0.5\n",
    "    # b1 is 10x1\n",
    "    b2 = np.random.rand( 10, 1 ) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# goes through Z element wise and returns a new matrix\n",
    "# 0 if Z[i] < 0\n",
    "# Z[i] if Z[i] > 0\n",
    "def ReLU( Z ):\n",
    "    return np.maximum( Z, 0 ) \n",
    "\n",
    "def softmax( Z ):\n",
    "    # Z -= np.max( Z )\n",
    "    A = np.exp( Z ) / sum( np.exp( Z ) )\n",
    "    return A\n",
    "\n",
    "def forward_prop( W1, b1, W2, b2, X ):\n",
    "    # Z1 is 10 x m\n",
    "    Z1 = W1.dot( X ) + b1\n",
    "    # A is 10 x m\n",
    "    A1 = ReLU( Z1 )\n",
    "    # Z2 is 10 x m\n",
    "    Z2 = W2.dot( A1 ) + b2\n",
    "    # A2 is 10 x m\n",
    "    A2 = softmax( Z2 )\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Y will be a matrix where each column has the\n",
    "def one_hot( Y ):\n",
    "    # Create new matrix of size: m x classes\n",
    "    # m = examples = Y.size\n",
    "    # classes = Y.max() + 1 = 10\n",
    "    one_hot_Y = np.zeros( ( Y.size, Y.max() + 1 ) )\n",
    "\n",
    "    # row - np.arrange( Y.size ) creates an array (the row)\n",
    "    # column - Y is the labels\n",
    "    one_hot_Y[ np.arange( Y.size ), Y ] = 1\n",
    "\n",
    "    # we want each column to be an example, so we transpose\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU( Z ):\n",
    "    return Z > 0\n",
    "\n",
    "def back_prop( Z1, A1, Z2, A2, W2, X, Y ):\n",
    "    m = Y.size\n",
    "    # print( \"m: \", m )\n",
    "    one_hot_Y = one_hot( Y )\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot( A1.T )\n",
    "    # print( \"back_prop: dZ2.shape: \", dZ2.shape )\n",
    "    db2 = 1 / m * np.sum( dZ2 )\n",
    "    dZ1 = W2.T.dot( dZ2 ) * deriv_ReLU( Z1 )\n",
    "    dW1 = 1 / m * dZ1.dot( X.T ) \n",
    "    db1 = 1 / m * np.sum( dZ1 )\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params( W1, b1, W2, b2, dW1, db1, dW2, db2, alpha ):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60631c2c-f405-4141-9dbe-488b4a98da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions( A2 ):\n",
    "    return np.argmax( A2, 0 )\n",
    "\n",
    "def get_accuracy( predictions, Y ):\n",
    "    print( predictions, Y )\n",
    "    return np.sum( predictions == Y ) / Y.size\n",
    "    \n",
    "def gradient_descent( X, Y, iterations, alpha ):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range( iterations ):\n",
    "        Z1, A1, Z2, A2 = forward_prop( W1, b1, W2, b2, X )\n",
    "        dW1, db1, dW2, db2 = back_prop( Z1, A1, Z2, A2, W2, X, Y )\n",
    "        W1, b1, W2, b2 = update_params( W1, b1, W2, b2, dW1, db1, dW2, db2, alpha )\n",
    "        if i % 25 == 0:\n",
    "            print( \"Iteration: \", i )\n",
    "            print( \"Accuracy: \", get_accuracy( get_predictions( A2 ), Y ))\n",
    "    return W1, b2, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "815c27c5-c26a-4f43-b301-477595039db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[0 3 6 ... 0 6 0] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.11174576271186441\n",
      "Iteration:  25\n",
      "[5 3 4 ... 2 6 5] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.23808474576271185\n",
      "Iteration:  50\n",
      "[5 5 4 ... 2 6 5] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.3560847457627119\n",
      "Iteration:  75\n",
      "[4 5 4 ... 2 8 5] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.46408474576271186\n",
      "Iteration:  100\n",
      "[4 7 7 ... 7 1 5] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.5748813559322034\n",
      "Iteration:  125\n",
      "[4 9 7 ... 7 1 5] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.6458983050847458\n",
      "Iteration:  150\n",
      "[4 9 7 ... 7 1 5] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.6897796610169491\n",
      "Iteration:  175\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.7197457627118644\n",
      "Iteration:  200\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.7416440677966102\n",
      "Iteration:  225\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.7596440677966102\n",
      "Iteration:  250\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.7743220338983051\n",
      "Iteration:  275\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.7854915254237288\n",
      "Iteration:  300\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.7954237288135593\n",
      "Iteration:  325\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8035254237288135\n",
      "Iteration:  350\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8110338983050848\n",
      "Iteration:  375\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8178135593220339\n",
      "Iteration:  400\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8235254237288135\n",
      "Iteration:  425\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.828864406779661\n",
      "Iteration:  450\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8338983050847457\n",
      "Iteration:  475\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8380169491525423\n",
      "Iteration:  500\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.841406779661017\n",
      "Iteration:  525\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8451864406779661\n",
      "Iteration:  550\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8479661016949153\n",
      "Iteration:  575\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8504915254237289\n",
      "Iteration:  600\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8531864406779661\n",
      "Iteration:  625\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8553050847457627\n",
      "Iteration:  650\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8571525423728814\n",
      "Iteration:  675\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8591186440677966\n",
      "Iteration:  700\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8607796610169491\n",
      "Iteration:  725\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8626440677966102\n",
      "Iteration:  750\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8640338983050847\n",
      "Iteration:  775\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8655084745762712\n",
      "Iteration:  800\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8670338983050847\n",
      "Iteration:  825\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8681525423728813\n",
      "Iteration:  850\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8694237288135593\n",
      "Iteration:  875\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.870593220338983\n",
      "Iteration:  900\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8715762711864407\n",
      "Iteration:  925\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8725084745762712\n",
      "Iteration:  950\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8733898305084746\n",
      "Iteration:  975\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8743898305084746\n",
      "Iteration:  1000\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8748474576271187\n",
      "Iteration:  1025\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8753389830508475\n",
      "Iteration:  1050\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8763050847457627\n",
      "Iteration:  1075\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.877\n",
      "Iteration:  1100\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8776779661016949\n",
      "Iteration:  1125\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.878406779661017\n",
      "Iteration:  1150\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8789830508474576\n",
      "Iteration:  1175\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8797627118644068\n",
      "Iteration:  1200\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8804406779661017\n",
      "Iteration:  1225\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8810169491525424\n",
      "Iteration:  1250\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8819830508474577\n",
      "Iteration:  1275\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.882728813559322\n",
      "Iteration:  1300\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8833220338983051\n",
      "Iteration:  1325\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8836610169491526\n",
      "Iteration:  1350\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8841186440677966\n",
      "Iteration:  1375\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8845254237288136\n",
      "Iteration:  1400\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8850847457627119\n",
      "Iteration:  1425\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8856949152542373\n",
      "Iteration:  1450\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8862033898305085\n",
      "Iteration:  1475\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.886728813559322\n",
      "Iteration:  1500\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8871016949152543\n",
      "Iteration:  1525\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8873728813559322\n",
      "Iteration:  1550\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8878474576271187\n",
      "Iteration:  1575\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8881864406779661\n",
      "Iteration:  1600\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8887627118644068\n",
      "Iteration:  1625\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8889830508474577\n",
      "Iteration:  1650\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8894237288135594\n",
      "Iteration:  1675\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8898983050847458\n",
      "Iteration:  1700\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8902542372881356\n",
      "Iteration:  1725\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8907627118644068\n",
      "Iteration:  1750\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8911864406779662\n",
      "Iteration:  1775\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8914915254237288\n",
      "Iteration:  1800\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8918813559322034\n",
      "Iteration:  1825\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8922033898305085\n",
      "Iteration:  1850\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8924406779661017\n",
      "Iteration:  1875\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8930677966101694\n",
      "Iteration:  1900\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8932881355932203\n",
      "Iteration:  1925\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.893593220338983\n",
      "Iteration:  1950\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8938983050847458\n",
      "Iteration:  1975\n",
      "[4 9 4 ... 7 1 3] [4 9 4 ... 7 4 3]\n",
      "Accuracy:  0.8944237288135594\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent( X_train, Y_train, 2000, 0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96578c5-b824-4ef5-8702-113946906415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
